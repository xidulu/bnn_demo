{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "from mxnet import np, npx\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn\n",
    "import mxnet.gluon.probability as mgp\n",
    "from mxnet.gluon.probability import StochasticBlock, StochasticSequential\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "npx.set_np()\n",
    "data_ctx = mx.cpu()\n",
    "model_ctx = mx.gpu(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size):\n",
    "    \"\"\"\n",
    "    Load MNIST\n",
    "    \"\"\"\n",
    "    mnist_train = gluon.data.vision.MNIST(train=True)\n",
    "    mnist_test = gluon.data.vision.MNIST(train=False)\n",
    "    num_worker = 4\n",
    "    transformer = gluon.data.vision.transforms.ToTensor()\n",
    "    return (gluon.data.DataLoader(mnist_train.transform_first(transformer),\n",
    "                                batch_size, shuffle=True,\n",
    "                                num_workers=num_worker),\n",
    "          gluon.data.DataLoader(mnist_test.transform_first(transformer),\n",
    "                                batch_size, shuffle=False,\n",
    "                                num_workers=num_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct A Bayesian dense layer using local reparameterization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalReparamDense(StochasticBlock):\n",
    "    def __init__(self, in_features, out_features, activation=None, flatten=True, dtype='float32'):\n",
    "        super(LocalReparamDense, self).__init__()\n",
    "        self._flatten = flatten\n",
    "        self.qw_x = None\n",
    "        self._in_features = in_features\n",
    "        self._out_features = out_features\n",
    "        # Parameter of weight\n",
    "        self.loc_w = gluon.Parameter('loc_w', shape=(out_features, in_features),\n",
    "                                    dtype=dtype)\n",
    "        self.scale_w = gluon.Parameter('log_scale_w', shape=(out_features, in_features),\n",
    "                                    dtype=dtype)\n",
    "        # Parameter of bias\n",
    "        self.bias = gluon.Parameter('bias', shape=(out_features,),\n",
    "                                    dtype=dtype)\n",
    "        if activation is not None:\n",
    "            self.act = gluon.nn.Activation(activation)\n",
    "        else:\n",
    "            self.act = None\n",
    "\n",
    "    @StochasticBlock.collectLoss\n",
    "    def hybrid_forward(self, F, x, loc_w, scale_w, bias):\n",
    "        # We use `fc` operator for matrix multiplication.\n",
    "        fc = F.npx.fully_connected\n",
    "        # Directly acquire parameter for A = dot(x, W)\n",
    "        # with local reparameterization trick:\n",
    "        qa_loc = fc(x, loc_w, bias=None, no_bias=True, num_hidden=self._out_features,\n",
    "                    flatten=self._flatten)\n",
    "        qa_scale = F.np.sqrt(fc(x ** 2, scale_w ** 2, bias=None, no_bias=True,\n",
    "                      num_hidden=self._out_features, flatten=self._flatten))\n",
    "        self.qw_x = mgp.Normal(\n",
    "            loc=qa_loc,\n",
    "            scale=qa_scale\n",
    "        )\n",
    "        # KL(qw_x || px), where px ~ N(0, 1)\n",
    "        kl = mgp.kl_divergence(self.qw_x, mgp.Normal(0, 1)).sum(-1)\n",
    "        self.add_loss(kl)\n",
    "        # Sampling from the network\n",
    "        fc_samples = self.qw_x.sample() + bias\n",
    "        if self.act is not None:\n",
    "            out = self.act(fc_samples)\n",
    "        else:\n",
    "            out = fc_samples\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST classfication with BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, n_epoch, train_iter, test_iter, baseline=False):\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                      {'learning_rate': .001})\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    loss_func = gluon.loss.SoftmaxCrossEntropyLoss(from_logits=(not baseline))\n",
    "    metric = mx.gluon.metric.Accuracy()\n",
    "    for epoch in tqdm_notebook(range(n_epoch), desc='epochs'):\n",
    "        epoch_loss = 0\n",
    "        metric.reset()\n",
    "        for batch in train_iter:\n",
    "            data = batch[0].as_in_context(model_ctx).reshape(-1, 28 * 28)\n",
    "            label = batch[1].as_in_context(model_ctx)\n",
    "            kl_loss = 0\n",
    "            with autograd.record():\n",
    "                logits = net(data)\n",
    "                classification_loss = loss_func(logits, label)\n",
    "                # `baseline` model stands for deterministic MLP\n",
    "                if baseline:\n",
    "                    loss = classification_loss\n",
    "                else:\n",
    "                    for layer_kl_loss in net.losses:\n",
    "                        kl_loss = kl_loss + layer_kl_loss[0]\n",
    "                    loss = classification_loss + kl_loss / data.shape[0]\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            epoch_loss += np.mean(classification_loss)\n",
    "        print(epoch_loss)\n",
    "        test_loss = 0\n",
    "        for batch in test_iter:\n",
    "            data = batch[0].as_in_context(model_ctx).reshape(-1, 28 * 28)\n",
    "            label = batch[1].as_in_context(model_ctx)\n",
    "            logits = net(data)\n",
    "            classification_loss = loss_func(logits, label)\n",
    "            test_loss += np.mean(classification_loss)\n",
    "            metric.update([label], [logits.as_nd_ndarray()])\n",
    "        name, acc = metric.get()\n",
    "        print('[Epoch %d] Training: %s=%f'%(epoch, name, acc))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.HybridSequential()\n",
    "mlp.add(nn.Dense(256, activation='relu'))\n",
    "mlp.add(nn.Dense(256, activation='relu'))\n",
    "mlp.add(nn.Dense(10))\n",
    "mlp.initialize(ctx=model_ctx)\n",
    "mlp.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_set, test_set = load_data(batch_size)\n",
    "train(\n",
    "    net=mlp,\n",
    "    n_epoch=1,\n",
    "    train_iter=train_set,\n",
    "    test_iter=test_set,\n",
    "    baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = StochasticSequential()\n",
    "bnn.add(LocalReparamDense(784, 256, activation='relu'))\n",
    "bnn.add(LocalReparamDense(256, 256, activation='relu'))\n",
    "bnn.add(LocalReparamDense(256, 10))\n",
    "bnn.initialize(ctx=model_ctx)\n",
    "bnn.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd39910ea904f828e4014c6cda12b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epochs', max=2.0, style=ProgressStyle(description_width='â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-21708.85\n",
      "[Epoch 0] Training: accuracy=0.913200\n",
      "-43544.555\n",
      "[Epoch 1] Training: accuracy=0.944400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_set, test_set = load_data(batch_size)\n",
    "train(\n",
    "    net=bnn,\n",
    "    n_epoch=2,\n",
    "    train_iter=train_set,\n",
    "    test_iter=test_set,\n",
    "    baseline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform classfication on white noise to demonstrate the advantages of a Bayesian neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random noise\n",
    "x = np.random.randn(28, 28)\n",
    "plt.imshow(x.asnumpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "bnn_prediction = npx.softmax(bnn(np.repeat(np.expand_dims(x, 0), 100, 0).as_in_context(model_ctx))).mean(0).asnumpy()\n",
    "mlp_prediction = npx.softmax(mlp(np.repeat(np.expand_dims(x, 0), 100, 0).as_in_context(model_ctx))).mean(0).asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [str(i) for i in range(10)]\n",
    "x = np.arange(len(labels)).asnumpy()  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, bnn_prediction, width, label='BNN prediction')\n",
    "rects2 = ax.bar(x + width/2, mlp_prediction, width, label='MLP prediction')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
